{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67c8560-ca61-4463-a17e-0dcaea9c7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "from hugchat.login import Login\n",
    "\n",
    "\n",
    "EMAIL = \"rowena.ravenbot@gmail.com\"\n",
    "PASSWD = \"z55eec7QV8b0\"\n",
    "cookie_path_dir = \"/Users/enyiomaosondu/personal/final-year-project/cookies/\"\n",
    "sign = Login(EMAIL, PASSWD)\n",
    "cookies = sign.login(cookie_dir_path=cookie_path_dir, save_cookies=True)\n",
    "\n",
    "chatbot = hugchat.ChatBot(cookies=cookies.get_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b00661-787a-4b13-a082-2248a98c260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_result = chatbot.chat(\"Hi!\").wait_until_done()\n",
    "message_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4a64ef-24eb-4e8a-adea-33249e39be00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je suis convaincu.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_result = chatbot.chat('Rephrase this to be shorter and give me only the result: Je crois en tout ce que nous faisons au quotidien.').wait_until_done()\n",
    "message_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fcd8edb-4d43-4ff4-baa4-c27636d97dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['sysctl', '-n', 'sysctl.proc_translated']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4All\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT4All(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morca-mini-3b-gguf2-q4_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/personal/final-year-project/env/lib/python3.10/site-packages/gpt4all/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CancellationError \u001b[38;5;28;01mas\u001b[39;00m CancellationError, Embed4All \u001b[38;5;28;01mas\u001b[39;00m Embed4All, GPT4All \u001b[38;5;28;01mas\u001b[39;00m GPT4All\n",
      "File \u001b[0;32m~/personal/final-year-project/env/lib/python3.10/site-packages/gpt4all/gpt4all.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncompleteRead, ProtocolError\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pyllmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CancellationError \u001b[38;5;28;01mas\u001b[39;00m CancellationError, EmbCancelCallbackType, EmbedResult \u001b[38;5;28;01mas\u001b[39;00m EmbedResult,\n\u001b[1;32m     24\u001b[0m                          LLModel, ResponseCallbackType, empty_response_callback)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Self, TypeAlias\n",
      "File \u001b[0;32m~/personal/final-year-project/env/lib/python3.10/site-packages/gpt4all/_pyllmodel.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Detect Rosetta 2\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m platform\u001b[38;5;241m.\u001b[39msystem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m platform\u001b[38;5;241m.\u001b[39mprocessor() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi386\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msysctl -n sysctl.proc_translated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m            Running GPT4All under Rosetta is not supported due to CPU feature requirements.\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m            Please install GPT4All in an environment that uses a native ARM64 Python interpreter.\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Find CUDA libraries from the official packages\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:524\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    525\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['sysctl', '-n', 'sysctl.proc_translated']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\", device='gpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc557b0-cf14-46e4-8824-83475a8f2598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
